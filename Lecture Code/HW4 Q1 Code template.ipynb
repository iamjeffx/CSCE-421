{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is a code template you can refer to for Question 1, you need to write standard_normalizer and PCA_sphereing function correctly to make this work, feel free to change this template if you need**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this and the following example we compare runs of gradient descent on various real datasets using a) the original input, b) standard normalized input, and c) PCA sphered input.  The `Python` function can be used to loop over three gradient descent runs using a single cost function with each version of the data loaded in.  Three steplength parameter inputs allow one to adjust and compare steplength choices for each run.\n",
    "\n",
    "As we saw when comparing standard normalization to original input in Sections 8.4, 9.4, and 10.3 we will typically find that a substantially larger steplength value can be used when comparing a run on original data to one on standard normalized data, and likewise when comparing a run on standard normalized data to one in which the input was first PCA sphered.  The intuition behind why this is possible - first detailed in Section 8.4.3 - is that PCA sphereing tends to make the contours of a cost function even more 'circular' than standard normalization, and the more circular a cost function's contours become the larger the steplength one can use because the gradient descent direction aligns more closely with the direction one must travel in to reach a true global minimum of a cost function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "def identity(x):\n",
    "    normalizer = lambda data: data\n",
    "    inverse_normalizer = lambda data: data\n",
    "    return normalizer,inverse_normalizer\n",
    "\n",
    "def compare_schemes(x,y,costname,countname,alpha1,alpha2,alpha3,max_its):     \n",
    "    # parameters for all gradient descent runs\n",
    "    C = len(np.unique(y))\n",
    "    if C == 2:\n",
    "        C-=1\n",
    "    \n",
    "    # create initialization\n",
    "    w = 0.0*np.random.randn(x.shape[0]+1,C)\n",
    "    \n",
    "    # gradient descent loop\n",
    "    cost_histories = []\n",
    "    count_histories = []\n",
    "    for transform,alpha_choice in zip([identity,standard_normalizer,PCA_sphereing],[alpha1,alpha2,alpha3]): \n",
    "        #### transform input data ####\n",
    "        # transform data\n",
    "        normalizer,inverse_normalizer = transform(x)\n",
    "\n",
    "        # normalize input\n",
    "        x_transformed = normalizer(x)\n",
    "        \n",
    "        #### make cost and misclassification counter based on transformed input ####\n",
    "        # create cost and counter\n",
    "        cost = cost_lib.choose_cost(x_transformed,y,costname)\n",
    "        count = cost_lib.choose_cost(x_transformed,y,countname)\n",
    "        \n",
    "        #### run gradient descent ####\n",
    "        # make run of gradient descent\n",
    "        weight_history,cost_history = optimizers.gradient_descent(cost,alpha_choice,max_its,w)\n",
    "        \n",
    "        # compute number of misclassifications\n",
    "        count_history = [count(v) for v in weight_history]\n",
    "        cost_histories.append(cost_history)\n",
    "        count_histories.append(count_history)\n",
    "    return cost_histories,count_histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we illustrate a run on each type of input using $50,000$ handwritten digits from the [MNIST dataset](http://scikit-learn.org/stable/datasets/index.html) - consisting of hand written digits between 0 and 9.  These images have been contrast normalized, a common pre-processing technique for image based data we discuss in Chapter 16. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick steplength values precisely as done in the previous dataset, and again find that we pick much larger values when comparing runs on the original to that of the standard normalized input, and this to the PCA sphered input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape =  (784, 70000)\n",
      "output shape =  (1, 70000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from autograd import numpy as np\n",
    "\n",
    "# import MNIST\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "# re-shape input/output data\n",
    "x = x.T\n",
    "y = np.array([int(v) for v in y])[np.newaxis,:]\n",
    "\n",
    "print(\"input shape = \" , x.shape)\n",
    "print(\"output shape = \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly sample input / output pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample indices\n",
    "num_sample = 50000\n",
    "inds = np.random.permutation(y.shape[1])[:num_sample]\n",
    "x_sample = x[:,inds]\n",
    "y_sample = y[:,inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create normalizer\n",
    "# normalizer,inverse_normalizer = standard_normalizer(x_sample.T)\n",
    "\n",
    "# # normalize input\n",
    "# x_sample = normalizer(x_sample.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run comparison module above\n",
    "alpha_orig = 10**(-5);  alpha_standard = 10**(-1); alpha_pca_sphered = 100;  costname = 'multiclass_softmax'; countname = 'multiclass_counter';\n",
    "max_its = 10\n",
    "cost_histories,count_histories = compare_schemes(x_sample,y_sample,costname,countname,alpha_orig,alpha_standard,alpha_pca_sphered,max_its)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the resulting cost function histories we can see how the run on standard normalized data converges rapidly in comparison to the raw data, and how the run on PCA sphered data converges even more rapidly still."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare cost / count histories\n",
    "static_plotter = superlearn.classification_static_plotter.Visualizer()\n",
    "static_plotter.plot_cost_histories(cost_histories,count_histories,start = 1,labels = ['original','standard','sphered'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
